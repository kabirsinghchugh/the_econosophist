+++
author = "kabir chugh"
date = 2017-09-02T14:33:02Z
description = "How are Language and Cognition related?"
draft = false
slug = "language-thoughts-metaphors-and-analogies"
title = "Language, Thoughts, Metaphors and Analogies..."
tags = ["Cognition","Linguistics","Philosophy"]
+++


<p><em>The colorless green vapor. </em></p>
<p><em>How much does your voice weigh?</em></p>
<p>The connection between Language and thought has been rigorously studied and speculated. Do we think through language? or language expresses thought? How are our thoughts affected when we don&#8217;t have any concept of a language? These are some interesting and important questions, but what is obvious from the first two statements is that language can go beyond thoughts into the realm of nonsense, but we have thoughts which language can&#8217;t articulate with all its words and dictionaries. So why this dual contradiction? Let&#8217;s explore!</p>
<p>The first two sentences. Carefully think of each. Can you think of a colorless green vapor? It’s a logical negation. An impossibility. However, nonsensical statements are surely possible, but what picture it produces in your mind? The weight of your voice? This effect is called a language illusion, which in our ordinary lives is not so explicit. Ludwig Wittgenstein, a logician philosopher claimed that all philosophy beyond scientific statements belonged to this realm of ‘non-sense’. This doesn’t mean that Language has a one up on thoughts since it encompasses thoughts and nonsense, but there are thoughts which are outside the ambit of language as well.</p>
<p>Consider this thought experiment (also attributed to Wittgenstein). Suppose you have two people, each with an identical white box, and an object inside which we assume to be different, which the other can’t see. (we call the people A and B and the objects X and Y respectively) A looks at X, thinks it’s a beetle and announces that he has a ‘beetle’. The beetle in A and B’s dictionary is called Z, so the box represents Z. That means, B looks at Y, thinks it’s a beetle and announces Z, which is his identical box. Both A and B use the same box, that is the word, but here comes the interesting part. The objects X and Y can be anything, either both beetles, beetle looking wasps, or something else, which they both call beetle but can’t see the fundamental distinction between X and Y, and hence thought gains a one up on Language. So when we use words such as <em>love, justice, </em>and freedom, different forms exist in different people with some similarities, but usually more different than we presume.</p>
<p>What is a chair? When you picture a chair, the familiar cushioned four legged furniture item comes to mind. However, a flat rock or a flat tree trunk are chairs? So a chair is something or anything we can sit on? Defined by its use? This presents a problem. Then what is a human being? Is a fetus a human being when it is not sentient nor looks like us? Then how come a fetus enjoys more human rights than say a chimpanzee? Our genetically closest being?</p>
<p>But one of my favorite language puzzles is sarcasm. It’s amazing on our ability to identify someone’s true meaning in some unrelated statement, sometimes the exact opposite of that meaning itself, and above that, the sarcastic person doesn’t try to make it any obvious of his sarcasm, as we know skill here is to be as subtle as possible, but what then divides the line of a true intentioned meaning and sarcasm, and how are we so good at detecting it?</p>
<p>So if B hurts itself and A asks, ‘Are you hurt?’ and B replies ‘No, I feel fabulous’ means the opposite. Thought gains one up on Language here, since the context becomes relevant. How important this is can be judged in AI, and how AI would recognize sarcasm in the future.</p>
<p>Similarly, reading between the lines, implied thoughts such as Netflix and Chilling which neither has to do anything with Netflix nor Chilling per se, but is used nonetheless. The question then is, why not be logically correct? Replying to Are you hurt a straight Yes or I would like to have sex with you, are not used because of the earlier language box example. Netflix and Chill can be a box which can either be X, I want to have sex with you, or Y, I really want to Netflix and Chill. B here can’t tell whether A Means X or Y, while A knows B can choose X or Y, and if chooses Y, it’s a safety outlet to mean that No, I do not want to have sex with you. These are the Language Games we play daily without even realizing it.</p>
<p>This leads to a final section on why it’s not all that bad to be obfuscatory in a complex argument. Suppose an author has in his mind an idea X, which he wants his audience to get as he conceives of it. The expert then uses language in such a way that he projects X through a combination of words Y, which the receiver decodes easily, with minimum effort and retains the original signal X. But an expert is not a computer. An expert can be a poet, which is the ultimate form of aesthetic enjoyment in decoding a signal, but say this expert first distorts his idea X intentionally into X’, which contains fragments of X along with other structural distortions, which are also not intentionally encoded, but the expert is fully aware of this. We call this a mutation, and though this mutation can be anything, the expert has some idea on ways this can be decoded. The reader then struggles with the decoding process. Y reveals fragments of X&#8217;, but these fragments are not enough to form coherence, so here B adds his own data to X’’(what he gets from X’ fragments). B then completes the picture to (X’’+D) which we call B’s Interpretation of Y, as it makes sense to B’s life. The expert is somebody who is knowingly good at this process.</p>



